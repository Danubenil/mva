{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "638350c1-852c-48fe-b03e-c0a6d424f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a0e721-4a3c-4bfe-9045-0b9e60ceb27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self ,id):\n",
    "        self.id = id\n",
    "        self.word2index = {\"<SOS>\" : 0, \"<EOS>\": 1}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0 : \"<SOS>\", 1: \"<EOS>\"}\n",
    "        self.nwords = 2\n",
    "        pass\n",
    "    def word2vec(self, word):\n",
    "        pass\n",
    "    def vec2word(self, vec):\n",
    "        pass\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.nwords\n",
    "            self.index2word[self.nwords] = word\n",
    "            self.nwords += 1\n",
    "            self.word2count[word] = 0\n",
    "        self.word2count[word] += 1\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2450c5a6-4f3b-41a1-a6f4-65a00bb7d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eng-fra.txt\", \"r\") as f:\n",
    "    lines = f.read().strip().split(\"\\n\")\n",
    "    pairs = [[normalizeString(pair) for pair in l.split(\"\\t\")] for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0288ca86-71c7-4b86-b7c8-184851130a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = Lang(\"fr\")\n",
    "en = Lang(\"en\")\n",
    "for pair in pairs:\n",
    "    fr.addSentence(pair[1])\n",
    "    en.addSentence(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29ba23f4-72e4-4930-ab0c-98025dcb9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "fr_prefixes = (\n",
    "    \"je suis\",\n",
    "    \"il est\", \"c est\",\n",
    "    \"elle est\", \"ce sont\",\n",
    "    \"vous etes\", \"tu es\",\n",
    "    \"nous sommes\", \"on est\",\n",
    "    \"ils sont\", \"elles sont\"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(fr_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6659d3d8-0a02-44f8-96a2-7a75ceeceb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eng-fra.txt\", \"r\") as f:\n",
    "    lines = f.read().strip().split(\"\\n\")\n",
    "    pairs = filterPairs([[normalizeString(pair) for pair in l.split(\"\\t\")] for l in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e224dc15-1ebc-4b70-83bc-8e755814f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = Lang(\"fr\")\n",
    "en = Lang(\"en\")\n",
    "for pair in pairs:\n",
    "    fr.addSentence(pair[1])\n",
    "    en.addSentence(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f29222f2-9286-48f3-8108-4d4f615f7f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3223"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en.nwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a148a67-1a34-4198-a22e-139a6243164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, nlayers, dropout = 0.3):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nlayers = nlayers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(input_size , embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, nlayers, dropout = dropout, batch_first = True)\n",
    "    def forward(self, x):\n",
    "        # [1 x seq_length ]\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # [1 x seq_length x embedding_size]\n",
    "        outputs, (hidden, cell) =  self.rnn(embedding)\n",
    "        return (hidden, cell)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, nlayers, output_size, dropout = 0.3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nlayers = nlayers\n",
    "        self.output_size = output_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, nlayers, dropout = dropout, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward_step(self, x, hidden, cell, teacher = None):\n",
    "        # x : 1 x 1\n",
    "        # 1 word at a time\n",
    "        x = self.relu(self.dropout(self.embedding(x)))\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(x, )\n",
    "        # output : [1 x N x hidden_size]\n",
    "        preds = self.fc(output)\n",
    "\n",
    "        # 1 x N x length_vocab\n",
    "        preds = preds.squeeze(0)\n",
    "        \n",
    "        return preds, hidden, cell\n",
    "\n",
    "    def forward(self,  encoder_hidden, encoder_cell):\n",
    "        batch_size = encoder_hidden.size(1)\n",
    "        x = torch.empty(1,1,  device = device, dtype = torch.long,).fill_(0)\n",
    "        \n",
    "        hidden, cell = encoder_hidden, encoder_cell\n",
    "        outputs = []\n",
    "        for i in range(MAX_LENGTH):\n",
    "            output, hidden, cell = self.forward_step(x, hidden, cell)\n",
    "            outputs.append(output)\n",
    "            x = torch.argmax(output).unsqueeze(0).unsqueeze(0)\n",
    "            #x = top.squeeze(-1).detach()\n",
    "        outputs = torch.cat(outputs, dim = 0)\n",
    "        outputs = F.log_softmax(outputs, dim = -1)\n",
    "        return outputs\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, encoder_hidden_size, decoder_hidden_size, nlayers_encoder, nlayers_decoder, output_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_size, embedding_size, encoder_hidden_size, nlayers_encoder)\n",
    "        self.decoder = Decoder(output_size, embedding_size, decoder_hidden_size, nlayers_decoder, output_size)\n",
    "    def forward(self, x):\n",
    "        hidden, cell = self.encoder(x)\n",
    "        return self.decoder(hidden, cell)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b760b46c-b12d-4a40-b9ab-7eb28aa13393",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(3, 10, 4, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7d9f5e2-9758-4793-9eb2-f3a41f1e96e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc(torch.tensor([[1, 2,0], [1, 0, 0]], device =device))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "078b64d1-4763-4dba-9f7e-39feace0290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y= [], []\n",
    "for pair in pairs:\n",
    "    english_words = pair[0].split(\" \")\n",
    "    french_words = pair[1].split(\" \")\n",
    "    english_indexes = [0]\n",
    "    french_indexes = [0]\n",
    "    for word in english_words:\n",
    "        english_indexes.append(en.word2index[word])\n",
    "    for word in french_words:\n",
    "        french_indexes.append(fr.word2index[word])\n",
    "    french_indexes.append(1)\n",
    "    X.append(torch.tensor(english_indexes, device = device))\n",
    "    Y.append(torch.tensor(french_indexes, device = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a382b6cc-2184-4e45-b566-c2e2c44383df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4478])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0dad7d2-9d82-4820-8ea5-d8ce6c73be5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                             | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 4478]) torch.Size([1, 5, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [1, 4478], got [1, 5, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 13\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:1295\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3494\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3493\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [1, 4478], got [1, 5, 1]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "model = Seq2Seq(en.nwords, 256, 128, 128, 2, 2, fr.nwords).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_epochs = 3\n",
    "flatten = nn.Flatten()\n",
    "for i in tqdm(range(n_epochs)):\n",
    "    for x, y in zip(X, Y):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        outputs = outputs.view(-1, outputs.size(-1)\n",
    "        y = y.view(-1)\n",
    "        print(outputs.shape, y.shape)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f6e3782-2128-4d3c-b175-377439c6a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from string import ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "95fa68b9-089e-4d01-976a-a9b4b5bb1362",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"./data/names\"\n",
    "lang2label = {}\n",
    "index = 0\n",
    "names = {}\n",
    "for file in os.listdir(dir):\n",
    "    lang, _ = file.split(\".\")\n",
    "    lang2label[lang] = torch.tensor(index)\n",
    "    names[lang] = []\n",
    "    with open(dir + \"/\" + file, \"r\") as f:\n",
    "        for line in f:\n",
    "            names[lang].append(unicodeToAscii(line.strip()))\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a3974865-1771-4045-87e8-4d7763848f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx = {letter: i for i, letter in enumerate(ascii_letters + \" .,:;-'\")}\n",
    "num_letters = len(char2idx); num_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "754b877e-a582-480e-b487-c78255c19620",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_langs = len(lang2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "59688e9e-279a-4183-b041-96a9b578bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name2tensor(name):\n",
    "    res = torch.zeros(len(name), 1, num_letters, dtype = torch.long)\n",
    "    for i in range(len(name)):\n",
    "        res[i, 0, char2idx[name[i]]] = 1\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b0e019b7-75af-43c9-90e1-c3526223c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset():\n",
    "    dir = \"./data/names\"\n",
    "    lang2label = {}\n",
    "    index = 0\n",
    "    names = {}\n",
    "    X, y = [], []\n",
    "    for file in os.listdir(dir):\n",
    "        lang, _ = file.split(\".\")\n",
    "        lang2label[lang] = torch.tensor(index)\n",
    "        names[lang] = []\n",
    "        with open(dir + \"/\" + file, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = normalizeString(line)\n",
    "                X.append(name2tensor(line))\n",
    "\n",
    "                y.append(torch.tensor(index))\n",
    "        index += 1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b4c9d2-7d4e-4383-8028-dc4556e31a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = construct_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5ab09978-ae1d-4785-b3b0-d6173c9aea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "index_train, index_test = train_test_split(range(len(X)), test_size = 0.1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a14d8f27-b6ed-4dcf-a9b8-545c8b82abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = [], []\n",
    "X_test, y_test = [], []\n",
    "for i in index_train:\n",
    "    X_train.append(X[i])\n",
    "    y_train.append(y[i])\n",
    "for j in index_test:\n",
    "    X_test.append(X[j])\n",
    "    y_test.append(y[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "828959f1-152e-4de7-a6ba-2e143bf54316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.in2output = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x, hidden = None):\n",
    "        batch_size = x.size(0)\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(batch_size, self.hidden_size)\n",
    "        combined = torch.cat((x, hidden), dim = 1)\n",
    "        hidden = self.relu(self.in2hidden(combined))\n",
    "        output = self.in2output(combined)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.candidate_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.filter_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.in2output = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x, hidden = None, cell = None):\n",
    "        batch_size = x.size(0)\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(batch_size, self.hidden_size)\n",
    "            cell = torch.zeros(batch_size, self.hidden_size)\n",
    "        combined = torch.cat((x, hidden), dim = 1)\n",
    "        cell = cell * self.sigmoid(self.forget_gate(combined))\n",
    "        cell = cell + self.sigmoid(self.input_gate(combined)) * self.tanh(self.candidate_gate(combined))\n",
    "        hidden = self.sigmoid(self.filter_gate(combined)) * self.tanh(cell)\n",
    "        output = self.in2output(combined)\n",
    "        return output, (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "7c575c5d-5fcc-4696-b263-b4ad988c96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = LSTM(3, 32, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "1ff34207-0bab-4dea-835e-a6d415a46750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [07:15<00:00, 217.59s/it]\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = LSTM(num_letters, 256, num_langs) \n",
    "optimizer = torch.optim.Adam(params = model.parameters(),  lr = lr)\n",
    "n_epochs = 2\n",
    "losses = []\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(n_epochs)):\n",
    "    loss_total = 0\n",
    "    for j in range(len(X_train)):\n",
    "        \n",
    "        x, y = X_train[i], y_train[i]\n",
    "        optimizer.zero_grad()\n",
    "        hidden = None\n",
    "        cell = None\n",
    "        for c in x:\n",
    "            output, (hidden, cell) = model(c, hidden, cell)\n",
    "        \n",
    "        loss = criterion(output, y.unsqueeze(0))\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        loss_total += loss.item() / len(X_train)\n",
    "    losses.append(loss_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "584d3f67-4cf2-4f6c-b820-49ef1f56e2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-7.6276e-03, -1.1010e-01, -5.7426e-03,  3.2927e-02, -7.8736e-02,\n",
       "          -7.9761e-02,  7.5931e-02, -4.1672e-03,  5.3505e-03, -4.5437e-02,\n",
       "          -1.2615e-02, -4.1347e-02, -7.3037e-02, -7.2559e-03, -5.3806e-02,\n",
       "          -2.1059e-02, -7.8413e-02, -9.2925e-05]], grad_fn=<AddmmBackward0>),\n",
       " (tensor([[ 0.0325, -0.0264,  0.0376, -0.0371, -0.0182,  0.0233, -0.0119, -0.0261,\n",
       "            0.0038, -0.0226,  0.0166, -0.0223,  0.0284,  0.0090, -0.0232, -0.0061,\n",
       "            0.0059,  0.0270,  0.0019, -0.0074, -0.0058,  0.0301, -0.0155,  0.0329,\n",
       "            0.0037, -0.0055, -0.0123,  0.0144,  0.0225, -0.0341,  0.0375, -0.0221,\n",
       "            0.0273, -0.0273, -0.0389, -0.0065,  0.0219,  0.0377, -0.0286,  0.0008,\n",
       "           -0.0148, -0.0331, -0.0007, -0.0207, -0.0147,  0.0056, -0.0044, -0.0183,\n",
       "           -0.0132, -0.0323,  0.0112, -0.0235, -0.0274, -0.0292,  0.0203,  0.0260,\n",
       "           -0.0134, -0.0244,  0.0326,  0.0261,  0.0063, -0.0257, -0.0224, -0.0159,\n",
       "            0.0134,  0.0045,  0.0232,  0.0114,  0.0078,  0.0149, -0.0273,  0.0128,\n",
       "           -0.0273, -0.0371, -0.0106, -0.0285, -0.0216,  0.0136, -0.0262, -0.0226,\n",
       "            0.0066,  0.0097,  0.0042, -0.0257, -0.0220, -0.0263, -0.0116,  0.0208,\n",
       "            0.0146, -0.0239, -0.0291,  0.0158, -0.0231, -0.0388, -0.0088,  0.0359,\n",
       "            0.0143, -0.0016,  0.0004,  0.0049,  0.0200, -0.0224, -0.0231,  0.0202,\n",
       "            0.0005, -0.0354,  0.0169, -0.0209, -0.0216, -0.0107, -0.0120,  0.0226,\n",
       "            0.0010, -0.0050, -0.0293, -0.0282,  0.0074, -0.0108, -0.0104,  0.0146,\n",
       "            0.0367, -0.0276, -0.0145,  0.0252, -0.0030,  0.0164,  0.0145,  0.0177,\n",
       "           -0.0159,  0.0202, -0.0148,  0.0080,  0.0117,  0.0340,  0.0376, -0.0148,\n",
       "           -0.0123, -0.0181,  0.0264,  0.0155, -0.0129, -0.0005, -0.0239, -0.0237,\n",
       "            0.0009,  0.0126,  0.0059, -0.0285, -0.0124, -0.0044, -0.0175,  0.0047,\n",
       "            0.0061,  0.0378,  0.0362, -0.0243,  0.0337, -0.0106,  0.0141,  0.0093,\n",
       "            0.0350,  0.0118,  0.0030,  0.0196, -0.0226,  0.0146,  0.0110,  0.0274,\n",
       "           -0.0024, -0.0075,  0.0313,  0.0281,  0.0233, -0.0236, -0.0334, -0.0141,\n",
       "           -0.0076, -0.0104, -0.0040,  0.0119, -0.0159, -0.0153,  0.0160, -0.0185,\n",
       "            0.0342,  0.0130, -0.0106,  0.0181, -0.0241,  0.0212, -0.0288,  0.0363,\n",
       "           -0.0271,  0.0285,  0.0246, -0.0160, -0.0348, -0.0296,  0.0099, -0.0256,\n",
       "            0.0143, -0.0121,  0.0179, -0.0116, -0.0044, -0.0269,  0.0246,  0.0054,\n",
       "            0.0279,  0.0074, -0.0249, -0.0135,  0.0037,  0.0052,  0.0349, -0.0187,\n",
       "           -0.0127,  0.0278,  0.0048, -0.0238, -0.0135, -0.0249, -0.0086,  0.0275,\n",
       "            0.0135, -0.0184, -0.0281, -0.0276,  0.0104,  0.0217, -0.0098, -0.0140,\n",
       "           -0.0272,  0.0290, -0.0241,  0.0352, -0.0072,  0.0271, -0.0098, -0.0222,\n",
       "           -0.0081, -0.0151, -0.0021,  0.0230,  0.0256, -0.0157, -0.0020, -0.0237,\n",
       "            0.0025,  0.0099,  0.0211, -0.0170, -0.0061,  0.0180, -0.0382,  0.0113]],\n",
       "         grad_fn=<MulBackward0>),\n",
       "  tensor([[ 0.0639, -0.0524,  0.0704, -0.0739, -0.0353,  0.0442, -0.0231, -0.0534,\n",
       "            0.0077, -0.0437,  0.0322, -0.0434,  0.0555,  0.0177, -0.0466, -0.0124,\n",
       "            0.0118,  0.0538,  0.0037, -0.0143, -0.0115,  0.0601, -0.0310,  0.0671,\n",
       "            0.0071, -0.0112, -0.0243,  0.0283,  0.0456, -0.0688,  0.0697, -0.0443,\n",
       "            0.0545, -0.0523, -0.0755, -0.0130,  0.0430,  0.0726, -0.0555,  0.0016,\n",
       "           -0.0295, -0.0625, -0.0014, -0.0389, -0.0288,  0.0113, -0.0090, -0.0350,\n",
       "           -0.0258, -0.0635,  0.0215, -0.0460, -0.0536, -0.0578,  0.0385,  0.0511,\n",
       "           -0.0266, -0.0488,  0.0643,  0.0511,  0.0124, -0.0525, -0.0436, -0.0302,\n",
       "            0.0274,  0.0091,  0.0450,  0.0221,  0.0152,  0.0284, -0.0539,  0.0262,\n",
       "           -0.0521, -0.0714, -0.0210, -0.0553, -0.0428,  0.0257, -0.0518, -0.0442,\n",
       "            0.0128,  0.0187,  0.0086, -0.0482, -0.0429, -0.0518, -0.0230,  0.0422,\n",
       "            0.0284, -0.0454, -0.0587,  0.0312, -0.0459, -0.0736, -0.0176,  0.0705,\n",
       "            0.0289, -0.0032,  0.0008,  0.0101,  0.0389, -0.0455, -0.0460,  0.0397,\n",
       "            0.0009, -0.0734,  0.0325, -0.0415, -0.0435, -0.0219, -0.0240,  0.0426,\n",
       "            0.0020, -0.0099, -0.0578, -0.0573,  0.0147, -0.0207, -0.0207,  0.0287,\n",
       "            0.0751, -0.0533, -0.0271,  0.0481, -0.0061,  0.0318,  0.0283,  0.0356,\n",
       "           -0.0302,  0.0399, -0.0288,  0.0162,  0.0241,  0.0702,  0.0707, -0.0281,\n",
       "           -0.0240, -0.0354,  0.0512,  0.0309, -0.0254, -0.0010, -0.0462, -0.0462,\n",
       "            0.0017,  0.0244,  0.0117, -0.0569, -0.0242, -0.0089, -0.0349,  0.0093,\n",
       "            0.0121,  0.0740,  0.0725, -0.0477,  0.0675, -0.0208,  0.0274,  0.0184,\n",
       "            0.0663,  0.0230,  0.0061,  0.0387, -0.0434,  0.0288,  0.0218,  0.0533,\n",
       "           -0.0047, -0.0147,  0.0623,  0.0576,  0.0460, -0.0446, -0.0647, -0.0285,\n",
       "           -0.0152, -0.0208, -0.0078,  0.0237, -0.0325, -0.0301,  0.0317, -0.0372,\n",
       "            0.0647,  0.0259, -0.0216,  0.0344, -0.0480,  0.0425, -0.0543,  0.0716,\n",
       "           -0.0556,  0.0546,  0.0486, -0.0305, -0.0710, -0.0592,  0.0195, -0.0498,\n",
       "            0.0286, -0.0235,  0.0357, -0.0218, -0.0093, -0.0538,  0.0479,  0.0110,\n",
       "            0.0563,  0.0150, -0.0482, -0.0269,  0.0072,  0.0102,  0.0665, -0.0380,\n",
       "           -0.0259,  0.0568,  0.0096, -0.0474, -0.0259, -0.0472, -0.0176,  0.0549,\n",
       "            0.0253, -0.0346, -0.0542, -0.0540,  0.0201,  0.0421, -0.0194, -0.0271,\n",
       "           -0.0519,  0.0571, -0.0476,  0.0682, -0.0139,  0.0548, -0.0196, -0.0452,\n",
       "           -0.0161, -0.0285, -0.0041,  0.0460,  0.0498, -0.0303, -0.0040, -0.0461,\n",
       "            0.0050,  0.0200,  0.0408, -0.0343, -0.0125,  0.0343, -0.0731,  0.0222]],\n",
       "         grad_fn=<AddBackward0>)))"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "814b055d-ef9d-4c62-80f9-ce22934b47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = X_train[0]\n",
    "hidden = None\n",
    "for c in seq:\n",
    "    output, (hidden, cell) = model(c, hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "01e6f30c-99b3-4acb-b2e3-5f83f8da5a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c90b4cfc-09a8-42a9-a2bc-1331845df8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0020898328232687472, 0.0]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "7c4b8230-f0c8-4978-b041-9106093877b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "acc = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(X_train)):\n",
    "        x, y = X_train[i], y_train[i]\n",
    "        hidden = None\n",
    "        cell = None\n",
    "        for c in x:\n",
    "            output, (hidden, cell) = model(c, hidden, cell)\n",
    "        label_predicted = torch.argmax(output)\n",
    "        acc += (label_predicted.item() == y.item())\n",
    "    acc /= len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "65de7f8b-4bf8-41d2-92c5-b1c39d2a13c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4692239566035647"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9e64a426-9482-4815-9d99-dc4d366cbeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001673747174622966, 0.0]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc9e095-2c14-46ab-8d57-66d9a88aa393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Signal",
   "language": "python",
   "name": "signal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
