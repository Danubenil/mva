{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdval7tUZwdZ"
   },
   "source": [
    "# Training a tokenizer for code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CorBhMaiZwdb"
   },
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CPGVVOEHZwdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /home/danube/.local/lib/python3.10/site-packages (4.39.3)\n",
      "Requirement already satisfied: filelock in /home/danube/.local/lib/python3.10/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/danube/.local/lib/python3.10/site-packages (from datasets) (1.25.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/danube/.local/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/danube/.local/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/danube/.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/danube/.local/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/danube/.local/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/danube/.local/lib/python3.10/site-packages (from datasets) (3.10.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/danube/.local/lib/python3.10/site-packages (from datasets) (0.24.5)\n",
      "Requirement already satisfied: packaging in /home/danube/.local/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/danube/.local/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/danube/.local/lib/python3.10/site-packages (from transformers[sentencepiece]) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/danube/.local/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/danube/.local/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.4.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/danube/.local/lib/python3.10/site-packages (from transformers[sentencepiece]) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /home/danube/.local/lib/python3.10/site-packages (from transformers[sentencepiece]) (5.27.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/danube/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.3.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/danube/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/danube/.local/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/danube/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/danube/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/danube/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/danube/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/danube/.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/danube/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/danube/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/danube/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/danube/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/danube/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/danube/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/danube/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, dill, multiprocess, datasets, evaluate\n",
      "Successfully installed datasets-3.3.2 dill-0.3.8 evaluate-0.4.3 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to train a tokenizer for code.\n",
    "We'll use this dataset, consisting of pairs (comment, code):\n",
    "https://huggingface.co/datasets/code-search-net/code_search_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mR5Hw12jZwde",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c8c3a7db054b8ba947ec85c7b78475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "python.zip:   0%|          | 0.00/941M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f01216296b47cf995d17ee66a89a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd5dd29b5df456fba816ed40fd3ae6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddb83bd02d74e38b40847dc30ef1e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This can take a few minutes to load!\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QS7wdF3pZwdf",
    "outputId": "07b90300-e5d8-45b0-d180-8e05ed8ca9ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 412178\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pszTPASnZwdg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def _update_media_data(self):\n",
      "        \"\"\"\n",
      "        Update media data for playing devices.\n",
      "\n",
      "        Internal method which queries device via HTTP to update media\n",
      "        information (title, artist, etc.) and URL of cover image.\n",
      "        \"\"\"\n",
      "        # Use different query URL based on selected source\n",
      "        if self._input_func in self._netaudio_func_list:\n",
      "            try:\n",
      "                root = self.get_status_xml(self._urls.netaudiostatus)\n",
      "            except (ValueError, requests.exceptions.RequestException):\n",
      "                return False\n",
      "\n",
      "            # Get the relevant tags from XML structure\n",
      "            for child in root:\n",
      "                if child.tag == \"szLine\":\n",
      "                    if (self._title != html.unescape(child[1].text) if (\n",
      "                            child[1].text is not None) else None or\n",
      "                            self._artist != html.unescape(child[2].text) if (\n",
      "                                child[2].text is not None) else None or\n",
      "                            self._album != html.unescape(child[4].text) if (\n",
      "                                child[4].text is not None) else None):\n",
      "                        # Refresh cover with a new time stamp for media URL\n",
      "                        # when track is changing\n",
      "                        self._image_url = (ALBUM_COVERS_URL.format(\n",
      "                            host=self._host, port=self._receiver_port,\n",
      "                            time=int(time.time())))\n",
      "                        # On track change assume device is PLAYING\n",
      "                        self._state = STATE_PLAYING\n",
      "                    self._title = html.unescape(child[1].text) if (\n",
      "                        child[1].text is not None) else None\n",
      "                    self._artist = html.unescape(child[2].text) if (\n",
      "                        child[2].text is not None) else None\n",
      "                    self._album = html.unescape(child[4].text)if (\n",
      "                        child[4].text is not None) else None\n",
      "                    self._band = None\n",
      "                    self._frequency = None\n",
      "                    self._station = None\n",
      "\n",
      "        elif self._input_func == \"Tuner\" or self._input_func == \"TUNER\":\n",
      "            try:\n",
      "                root = self.get_status_xml(self._urls.tunerstatus)\n",
      "            except (ValueError, requests.exceptions.RequestException):\n",
      "                return False\n",
      "\n",
      "            # Get the relevant tags from XML structure\n",
      "            for child in root:\n",
      "                if child.tag == \"Band\":\n",
      "                    self._band = child[0].text\n",
      "                elif child.tag == \"Frequency\":\n",
      "                    self._frequency = child[0].text\n",
      "\n",
      "            self._title = None\n",
      "            self._artist = None\n",
      "            self._album = None\n",
      "            self._station = None\n",
      "\n",
      "            # Assume Tuner is always PLAYING\n",
      "            self._state = STATE_PLAYING\n",
      "\n",
      "            # No special cover, using a static one\n",
      "            self._image_url = (\n",
      "                STATIC_ALBUM_URL.format(\n",
      "                    host=self._host, port=self._receiver_port))\n",
      "\n",
      "        elif self._input_func == \"HD Radio\" or self._input_func == \"HDRADIO\":\n",
      "            try:\n",
      "                root = self.get_status_xml(self._urls.hdtunerstatus)\n",
      "            except (ValueError, requests.exceptions.RequestException):\n",
      "                return False\n",
      "\n",
      "            # Get the relevant tags from XML structure\n",
      "            for child in root:\n",
      "                if child.tag == \"Artist\":\n",
      "                    self._artist = html.unescape(child[0].text) if (\n",
      "                        child[0].text is not None) else None\n",
      "                elif child.tag == \"Title\":\n",
      "                    self._title = html.unescape(child[0].text) if (\n",
      "                        child[0].text is not None) else None\n",
      "                elif child.tag == \"Album\":\n",
      "                    self._album = html.unescape(child[0].text) if (\n",
      "                        child[0].text is not None) else None\n",
      "                elif child.tag == \"Band\":\n",
      "                    self._band = html.unescape(child[0].text) if (\n",
      "                        child[0].text is not None) else None\n",
      "                elif child.tag == \"Frequency\":\n",
      "                    self._frequency = html.unescape(child[0].text) if (\n",
      "                        child[0].text is not None) else None\n",
      "                elif child.tag == \"StationNameSh\":\n",
      "                    self._station = html.unescape(child[0].text) if (\n",
      "                        child[0].text is not None) else None\n",
      "\n",
      "            # Assume Tuner is always PLAYING\n",
      "            self._state = STATE_PLAYING\n",
      "\n",
      "            # No special cover, using a static one\n",
      "            self._image_url = (\n",
      "                STATIC_ALBUM_URL.format(\n",
      "                    host=self._host, port=self._receiver_port))\n",
      "\n",
      "        # No behavior implemented, so reset all variables for that source\n",
      "        else:\n",
      "            self._band = None\n",
      "            self._frequency = None\n",
      "            self._title = None\n",
      "            self._artist = None\n",
      "            self._album = None\n",
      "            self._station = None\n",
      "            # Assume PLAYING_DEVICE is always PLAYING\n",
      "            self._state = STATE_PLAYING\n",
      "            # No special cover, using a static one\n",
      "            self._image_url = (\n",
      "                STATIC_ALBUM_URL.format(\n",
      "                    host=self._host, port=self._receiver_port))\n",
      "\n",
      "        # Test if image URL is accessable\n",
      "        if self._image_available is None and self._image_url is not None:\n",
      "            try:\n",
      "                imgres = requests.get(self._image_url, timeout=self.timeout)\n",
      "            except requests.exceptions.RequestException:\n",
      "                # No result set image URL to None\n",
      "                self._image_url = None\n",
      "            else:\n",
      "                if imgres.status_code == 200:\n",
      "                    self._image_available = True\n",
      "                else:\n",
      "                    _LOGGER.info('No album art available for your receiver')\n",
      "                    # No image available. Save this status.\n",
      "                    self._image_available = False\n",
      "                    #  Set image URL to None.\n",
      "                    self._image_url = None\n",
      "        # Already tested that image URL is not accessible\n",
      "        elif self._image_available is False:\n",
      "            self._image_url = None\n",
      "\n",
      "        # Finished\n",
      "        return True\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets[\"train\"][41][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Which of the two pieces of code below should you NOT run?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second piece of code creates a generator instead of a list, it's memory efficient, but can be used only once, and doesn't support indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZGBwloCAZwdh"
   },
   "outputs": [],
   "source": [
    "training_corpus = [\n",
    "    raw_datasets[\"train\"][i : i + 10000][\"whole_func_string\"]\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), 10000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZGBwloCAZwdh"
   },
   "outputs": [],
   "source": [
    "training_corpus = (\n",
    "    raw_datasets[\"train\"][i : i + 10000][\"whole_func_string\"]\n",
    "    for i in range(0, len(raw_datasets[\"train\"]), 10000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of generators is that they do not load anything into memory... but the problem is that they can be used only once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SSRoQjZ0Zwdh",
    "outputId": "fe7489e5-a725-45a1-9399-f06142f6f608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "gen = (i for i in range(10))\n",
    "print(list(gen))\n",
    "print(list(gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s why we define a function that returns a generator instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dQ8a4Z3JZwdi"
   },
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 10000):\n",
    "        samples = dataset[start_idx : start_idx + 10000]\n",
    "        yield samples[\"whole_func_string\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the GPT-2 tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2Ch6m2V_Zwdi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danube/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e4637d20c442b58e6d7fb6175cdf47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e759523a8cb4b0291673d33fb2e6ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae186248db7247ddaa4ad92f4717587a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16067e2b0104a9888ccd1148854be97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ff2598af6248d2b448d977a7549a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "old_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Figure out how many tokens `old_tokenizer` uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer uses :  50257  tokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizer uses : \", len(old_tokenizer.get_vocab()), \" tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IuZAgGY3Zwdi",
    "outputId": "5994892c-1389-4cdf-e817-a00911581c30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'Ġadd',\n",
       " '_',\n",
       " 'n',\n",
       " 'umbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'Ġb',\n",
       " '):',\n",
       " 'Ċ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ\"\"\"',\n",
       " 'Add',\n",
       " 'Ġthe',\n",
       " 'Ġtwo',\n",
       " 'Ġnumbers',\n",
       " 'Ġ`',\n",
       " 'a',\n",
       " '`',\n",
       " 'Ġand',\n",
       " 'Ġ`',\n",
       " 'b',\n",
       " '`',\n",
       " '.\"',\n",
       " '\"\"',\n",
       " 'Ċ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġ',\n",
       " 'Ġreturn',\n",
       " 'Ġa',\n",
       " 'Ġ+',\n",
       " 'Ġb']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "* What do the special symbols Ġ and Ċ denote?\n",
    "* Why is this tokenization not optimal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "* The special symbol Ġ signifies a space.\n",
    "* The special symbol Ċ signifies a new line.\n",
    "* Suboptimal tokenizer for the code : doesn't take into account the indentation.\n",
    "* Doesn't take into account entity name (with _)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a new tokenizer on our training corpus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the ancient tokens, and add new tokens from training_corpus until we get 52000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IZoDNfuZZwdi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, vocab_size=52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WtFHzYclZwdj",
    "outputId": "ed4dae8b-71e6-4b11-f083-b37f7adb741d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'Ġadd',\n",
       " '_',\n",
       " 'numbers',\n",
       " '(',\n",
       " 'a',\n",
       " ',',\n",
       " 'Ġb',\n",
       " '):',\n",
       " 'ĊĠĠĠ',\n",
       " 'Ġ\"\"\"',\n",
       " 'Add',\n",
       " 'Ġthe',\n",
       " 'Ġtwo',\n",
       " 'Ġnumbers',\n",
       " 'Ġ`',\n",
       " 'a',\n",
       " '`',\n",
       " 'Ġand',\n",
       " 'Ġ`',\n",
       " 'b',\n",
       " '`.\"\"\"',\n",
       " 'ĊĠĠĠ',\n",
       " 'Ġreturn',\n",
       " 'Ġa',\n",
       " 'Ġ+',\n",
       " 'Ġb']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(example)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: Why is this tokenization better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tokenization is better because it takes into account indentation. It also see the second part of the function's name. Finally, it considers the quotes of docstring as a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "cDYv9wXdZwdj",
    "outputId": "523ecca3-932a-47e3-a276-674deecbc0ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2O5-ovFQZwdj",
    "outputId": "f5c38c35-d692-43f0-bdbc-61f4b7be1437"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class',\n",
       " 'ĠLinear',\n",
       " 'Layer',\n",
       " '():',\n",
       " 'ĊĠĠĠ',\n",
       " 'Ġdef',\n",
       " 'Ġ__',\n",
       " 'init',\n",
       " '__(',\n",
       " 'self',\n",
       " ',',\n",
       " 'Ġinput',\n",
       " '_',\n",
       " 'size',\n",
       " ',',\n",
       " 'Ġoutput',\n",
       " '_',\n",
       " 'size',\n",
       " '):',\n",
       " 'ĊĠĠĠĠĠĠĠ',\n",
       " 'Ġself',\n",
       " '.',\n",
       " 'weight',\n",
       " 'Ġ=',\n",
       " 'Ġtorch',\n",
       " '.',\n",
       " 'randn',\n",
       " '(',\n",
       " 'input',\n",
       " '_',\n",
       " 'size',\n",
       " ',',\n",
       " 'Ġoutput',\n",
       " '_',\n",
       " 'size',\n",
       " ')',\n",
       " 'ĊĠĠĠĠĠĠĠ',\n",
       " 'Ġself',\n",
       " '.',\n",
       " 'bias',\n",
       " 'Ġ=',\n",
       " 'Ġtorch',\n",
       " '.',\n",
       " 'zeros',\n",
       " '(',\n",
       " 'output',\n",
       " '_',\n",
       " 'size',\n",
       " ')',\n",
       " 'ĊĊĠĠĠ',\n",
       " 'Ġdef',\n",
       " 'Ġ__',\n",
       " 'call',\n",
       " '__(',\n",
       " 'self',\n",
       " ',',\n",
       " 'Ġx',\n",
       " '):',\n",
       " 'ĊĠĠĠĠĠĠĠ',\n",
       " 'Ġreturn',\n",
       " 'Ġx',\n",
       " 'Ġ@',\n",
       " 'Ġself',\n",
       " '.',\n",
       " 'weights',\n",
       " 'Ġ+',\n",
       " 'Ġself',\n",
       " '.',\n",
       " 'bias',\n",
       " 'ĊĠĠĠĠ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"\"\"class LinearLayer():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weight = torch.randn(input_size, output_size)\n",
    "        self.bias = torch.zeros(output_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "    \"\"\"\n",
    "tokenizer.tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: Further evidence that this tokenization is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python's keywords are tokens : class, self, def "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary size : hyperparameter\n",
    "* Large : Sentences split in small number of tokens (large tokens).\n",
    "* Small: model can handle it better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word level tokenizer is the best.\n",
    "When words are split, we need multiple tokens to have semantic meaning.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a new tokenizer from an old one",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Signal",
   "language": "python",
   "name": "signal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
