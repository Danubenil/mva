{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b459c9ff-a9a1-468d-9da3-e0b7cdc258d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af1563b4-7068-4cb3-9ad4-888e4cc2dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocabulary_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.embedder = nn.Embedding(vocabulary_size, emb_dim)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.embedder(batch)\n",
    "\"\"\"\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocabulary_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.embedder = nn.Linear(vocabulary_size, emb_dim)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch = F.one_hot(batch, self.vocabulary_size).float()\n",
    "        return self.embedder(batch)\n",
    "\"\"\"\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_seq_len, emb_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.emb_dim = emb_dim\n",
    "        pe = torch.zeros(max_seq_len, emb_dim)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, emb_dim , 2):\n",
    "                pe[pos, i] = math.sin(pos / (1e5 ** (2 * i / emb_dim)))\n",
    "                if i + 1 >= emb_dim:\n",
    "                    break\n",
    "                pe[pos, i + 1]= math.cos(pos / (1e5 ** (2 * i / emb_dim)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = x + torch.autograd.Variable(self.pe[:,:seq_len, :], requires_grad=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92544db0-7964-4271-b53e-001ecb014c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding(3, 10)\n",
    "posEmb = PositionalEmbedding(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d296c57-0a78-49a4-8320-34c84bc458cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6244c77e-9290-4cff-aa19-02d30007551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = embedding(torch.tensor([[1, 1, 0, 2, 0]]))\n",
    "posEmb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c813023a-a35f-4bce-8ed8-933e90fa1918",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_input, n_head = 3):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_head = n_head\n",
    "        self.n_size = n_input // n_head\n",
    "        self.key = nn.Linear(n_input, self.n_size * n_head)\n",
    "        self.query = nn.Linear(n_input, self.n_size * n_head)\n",
    "        self.value = nn.Linear(n_input, self.n_size * n_head)\n",
    "        self.out = nn.Linear(n_input, n_input)\n",
    "    def forward(self, x):\n",
    "        key = self.key(x)\n",
    "        query = self.query(x)\n",
    "        value = self.value(x)\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        key = key.reshape(batch_size, seq_len, self.n_head, self.n_size) \n",
    "        query = query.reshape(batch_size, seq_len, self.n_head, self.n_size) \n",
    "        value = value.reshape(batch_size, seq_len, self.n_head, self.n_size)\n",
    "        key = key.transpose(1, 2)\n",
    "        query = query.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "        attention_weights = (query @ key.transpose(-2, - 1)) / math.sqrt(self.n_size)\n",
    "        attention_scores = nn.Softmax(dim = -1)(attention_weights)\n",
    "        output = attention_scores @ value\n",
    "        output = output.transpose(1, 2)\n",
    "        output = output.reshape(batch_size, seq_len, self.n_input)\n",
    "        output = self.out(output)\n",
    "        return output\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, n_input, n_head = 3):\n",
    "        super(MultiHeadCrossAttention, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_head = n_head\n",
    "        self.n_size = n_input // n_head\n",
    "        self.key = nn.Linear(n_input, self.n_size * n_head)\n",
    "        self.query = nn.Linear(n_input, self.n_size * n_head)\n",
    "        self.value = nn.Linear(n_input, self.n_size * n_head)\n",
    "        self.out = nn.Linear(n_input, n_input)\n",
    "    def forward(self, x_encoder, x_decoder):\n",
    "        key = self.key(x_encoder)\n",
    "        query = self.query(x_decoder)\n",
    "        value = self.value(x_encoder)\n",
    "        batch_size = x_encoder.size(0)\n",
    "        seq_len_encoder = x_encoder.size(1)\n",
    "        seq_len_decoder = x_decoder.size(1)\n",
    "        key = key.reshape(batch_size, seq_len_encoder, self.n_head, self.n_size) \n",
    "        query = query.reshape(batch_size, seq_len_decoder, self.n_head, self.n_size) \n",
    "        value = value.reshape(batch_size, seq_len_encoder, self.n_head, self.n_size)\n",
    "        key = key.transpose(1, 2)\n",
    "        query = query.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "        attention_weights = (query @ key.transpose(-2, - 1)) / math.sqrt(self.n_size)\n",
    "        attention_scores = nn.Softmax(dim = -1)(attention_weights)\n",
    "        output = attention_scores @ value\n",
    "        output = output.transpose(1, 2)\n",
    "        output = output.reshape(batch_size, seq_len_decoder, self.n_input)\n",
    "        output = self.out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "580d5d63-9a48-4072-8984-cea30f082852",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MultiHeadAttention(6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "febe38a9-da45-431d-81a0-12d20b53feb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[[0., 1, 1.5, 2, 3, 4], [4., 4., 3, 2.4, 1.8, 1]]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5cf934a-d3ae-464f-b896-3ee57cdfe80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3023, -0.4288,  0.9929,  1.5199,  0.1874,  1.3461],\n",
       "         [-0.4214, -0.4523,  1.0127,  1.4972,  0.0902,  1.4811]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(torch.tensor([[[0., 1, 1.5, 2, 3, 4], [4., 4., 3, 2.4, 1.8, 1]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6144c0d3-2b2c-48c7-83c8-1e652298a3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 6])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[[0., 1, 1.5, 2, 3, 4]], [[4., 4., 3, 2.4, 1.8, 1]]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a8c742c9-e04f-4d9f-8988-330afcc95147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_input, n_head):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(n_input, n_head)\n",
    "        self.layer_norm1 = nn.LayerNorm(n_input)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_input, n_input),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(n_input)\n",
    "        self.dropout1 = nn.Dropout(p = 0.2)\n",
    "        self.dropout2 = nn.Dropout(p = 0.2)\n",
    "    def forward(self, x):\n",
    "        x_with_attentions = self.dropout1(self.layer_norm1(self.attention(x)))\n",
    "        x = x + x_with_attentions\n",
    "        x_fc = self.dropout2(self.layer_norm2(self.fc(x)))\n",
    "        x = x + x_fc\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_size, max_len_seq, n_encoders = 2, n_head = 3):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.pos_encoder = PositionalEmbedding(max_len_seq, embedding_size)\n",
    "        self.embedding = Embedding(vocab_size, embedding_size)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_encoders):\n",
    "            self.layers.append(EncoderBlock(embedding_size, n_head))\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        len_seq = x.size(1)\n",
    "        emb = self.embedding(x)\n",
    "        emb = self.pos_encoder(emb)\n",
    "        res = emb\n",
    "        for layer in self.layers:\n",
    "            res = layer(res)\n",
    "        return res\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_input, n_head):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(n_input, n_head)\n",
    "        self.cross_attention = MultiHeadCrossAttention(n_input, n_head)\n",
    "        self.layer_norm1 = nn.LayerNorm(n_input)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_input, n_input),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(n_input)\n",
    "        self.layer_norm3 = nn.LayerNorm(n_input)\n",
    "        self.dropout1 = nn.Dropout(p = 0.2)\n",
    "        self.dropout2 = nn.Dropout(p = 0.2)\n",
    "        self.dropout3 = nn.Dropout(p = 0.2)\n",
    "    def forward(self, x_decoder, x_encoder):\n",
    "        x_with_attentions = self.dropout1(self.layer_norm1(self.attention(x_decoder)))\n",
    "        x = x_decoder + x_with_attentions\n",
    "        x_with_cross_attentions = self.dropout2(self.layer_norm2(self.cross_attention(x_encoder, x)))\n",
    "        x = x + x_with_cross_attentions\n",
    "        x_fc = self.dropout3(self.layer_norm3(self.fc(x)))\n",
    "        x = x + x_fc\n",
    "        return x\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,target_vocab_size, input_vocab_size, embedding_size, max_len_seq, n_decoders = 2, n_head = 3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.pos_encoder = PositionalEmbedding(max_len_seq, embedding_size)\n",
    "        self.embedding = Embedding(input_vocab_size, embedding_size)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.out = nn.Linear(embedding_size, target_vocab_size)\n",
    "        \n",
    "        for i in range(n_decoders):\n",
    "            self.layers.append(DecoderBlock(embedding_size, n_head))\n",
    "    def forward(self, x_decoder, x_encoder):\n",
    "        batch_size = x_encoder.size(0)\n",
    "        len_seq = x_encoder.size(1)\n",
    "        emb = self.embedding(x_decoder)\n",
    "        emb = self.pos_encoder(emb)\n",
    "        res = emb\n",
    "        for layer in self.layers:\n",
    "            res = layer(res, x_encoder)\n",
    "        return self.out(res)\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, target_vocab_size, embedding_size, max_len_seq, n_encoders = 2, n_decoders= 2, n_head = 3):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size = vocab_size, embedding_size = embedding_size,\n",
    "                               max_len_seq = max_len_seq, n_encoders = n_encoders, n_head = n_head)\n",
    "        self.decoder = Decoder(target_vocab_size = target_vocab_size, input_vocab_size = vocab_size, \n",
    "                               embedding_size = embedding_size, max_len_seq = max_len_seq, n_decoders = n_decoders,\n",
    "                               n_head = n_head)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.max_len_seq = max_len_seq\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "    def forward(self, x, target = None):\n",
    "        encoded = self.encoder(x)\n",
    "        batch_size = x.size(0)\n",
    "        if target is None:\n",
    "            target = torch.zeros(batch_size, 1, dtype = torch.int) \n",
    "        decoded = self.decoder(target, encoded)\n",
    "        return self.softmax(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b86f5e10-0767-4a97-b5bf-61f2e3cd8057",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Transformer(10, 20, 30, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9db83bf-d94e-423e-934a-126086caddd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ede2e34e-5161-4ea1-8d09-4ea85c8e5bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = Decoder(10, 10, 9, 20, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0facc10b-5a4f-4b9d-8ea2-fc1555703926",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(20, 9, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f9a87295-cbb0-4849-9ffd-ae20384acbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = t(torch.tensor([[1, 2, 3], [4, 5, 6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4c4b823b-4190-4947-8db4-20f69220a369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 20])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "491d0a42-c3f7-4192-a6ed-22a06b801f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = DecoderBlock(9, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "28348fdd-67af-4340-ac29-d58362dd7d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.6996, -0.0540, -0.6785,  1.8729,  2.5463,  2.1610,  0.5126,\n",
       "           2.8026, -1.9383],\n",
       "         [ 1.6356, -0.7436, -0.4365, -0.7483,  1.9983,  2.3589, -1.0551,\n",
       "           3.3037, -1.2795],\n",
       "         [ 2.3710,  0.7427, -0.6359,  2.6279, -0.0899,  3.2949, -0.8824,\n",
       "           0.0907, -1.6748]],\n",
       "\n",
       "        [[ 2.3579,  0.6638, -1.4764,  2.3927,  0.9059,  0.3122, -0.6362,\n",
       "           2.6056, -2.0278],\n",
       "         [ 1.3375,  0.1816, -2.2884, -0.0256,  0.0244,  0.3844, -0.8661,\n",
       "           2.7096, -1.0883],\n",
       "         [ 1.4010, -2.4964, -0.7134,  1.8807,  0.6373,  1.1526, -0.4668,\n",
       "           3.5641, -2.9647]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "a8856bbc-77ce-44ac-83f8-a3e3fadcb3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0523, 0.0027, 0.0343, 0.0366, 0.0273, 0.0695, 0.0621, 0.0055,\n",
       "          0.0454, 0.6644],\n",
       "         [0.0165, 0.0185, 0.0279, 0.0604, 0.0092, 0.1517, 0.1187, 0.0142,\n",
       "          0.0573, 0.5257],\n",
       "         [0.1876, 0.0133, 0.1345, 0.0163, 0.2110, 0.0795, 0.0503, 0.1099,\n",
       "          0.0325, 0.1651]],\n",
       "\n",
       "        [[0.2006, 0.0007, 0.0296, 0.0614, 0.0403, 0.0224, 0.1603, 0.0042,\n",
       "          0.1473, 0.3330],\n",
       "         [0.2278, 0.0144, 0.1093, 0.1420, 0.0250, 0.0488, 0.0267, 0.0125,\n",
       "          0.3145, 0.0789],\n",
       "         [0.0460, 0.0108, 0.0337, 0.0297, 0.0253, 0.0274, 0.1948, 0.0232,\n",
       "          0.0782, 0.5309]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec(torch.tensor([[1, 2, 3], [4, 5, 6]]), res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "dbbaf4a0-5524-4a18-8da8-9e7c9dc1d923",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2730920761.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[304], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Decoder(\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "Decoder("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eab774-8506-4f31-a363-038b6849621e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Signal",
   "language": "python",
   "name": "signal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
