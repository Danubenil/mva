lamorel_args:
  log_level: info
  allow_subgraph_use_whith_gradient: false
  distributed_setup_args:
    n_rl_processes: 1
    n_llm_processes: 1
  accelerate_args:
    config_file: ./accelerate/default_config.yaml
    machine_rank: 0
    main_process_ip: 127.0.0.1
    main_process_port: 29500
    num_machines: 1
  llm_args:
    model_type: seq2seq
    model_path: google/flan-t5-small
    pretrained: true
    minibatch_size: 192
    pre_encode_inputs: false
    load_in_4bit: true
    parallelism:
      use_gpu: true
      model_parallelism_size: 1
      synchronize_gpus_after_scoring: false
      empty_cuda_cache_after_scoring: false
rl_script_args:
  path: /home/danube/llm/projet/Quantifying-and-Mitigating-Prompt-Overfitting-6548/experiments/BabyAI-Text/main_pi.py
  seed: 1
  alpha: 0.25
  margin: 1
  t: 10
  ppo_epochs: 3
  lam: 0.99
  gamma: 0.99
  lr: 0.0001
  entropy_coef: 0.01
  value_loss_coef: 0.5
  clip_eps: 0.2
  max_grad_norm: 0.5
  minibatch_size: 125
  use_all_params_for_optim: false
  quantized_optimizer: false
  use_lora: true
  gradient_batch_size: 4
  gradient_minibatch_size: null
  lora_r: 32
  lora_alpha: 16
  lora_target_modules:
  - q
  - v
  - k
  - wi
  - wo
  - lm_head
  number_envs: 4
  max_ep_len: 200
  epochs: 500
  steps_per_epoch: 1000
  prompt_id: 0
  save_freq: 25
  output_dir: .
  loading_path: null
  transitions_buffer_len: 2
  name_environment: babyai_text
  task: BabyAI-GoToLocal-v0
  action_space:
  - turn_left
  - turn_right
  - go_forward
  - pick_up
  - drop
  - toggle
  twc_levels: null
  config_file: null
  train_mode: true
